{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "header1",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liang/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, restart the kernel (in the menubar, select Kernel $\\rightarrow$ Restart) and then run all cells (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says YOUR CODE HERE or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "header3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# HW 1 - Forms of Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q1",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Find the absolute error, relative error, and decimal precision (number of significant decimal digits) for the following $f$ and approximations $\\hat{f}$.  Note that here we may also mean precision as compared to $f$.  In these cases use the absolute error to help define $\\hat{f}$'s precision (each worth 5 points).\n",
    "\n",
    "**(a)** $f = \\pi$ and $\\hat{f} = 3.14$\n",
    "\n",
    "**(b)** $f = \\pi$ and $\\hat{f} = 22 / 7$\n",
    "\n",
    "**(c)** $f = \\log (n!)$ and $\\hat{f} = n~log(n) - n$ for $n = 5, ~~ 10, ~~ 100$ (Stirling's approximation)\n",
    "\n",
    "**(d)** $f = e^x$ and $\\hat{f} = T_n(x)$ where $T_n(x)$ is the Taylor polynomial approximation to $e^x$ expanded about $x = 0$.  Consider $N = 1, 2, 3$.  What vaule of $N$ is required for this approximation to be good to 6 digits of decimal precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A1",
     "locked": false,
     "points": 20,
     "solution": true
    }
   },
   "source": [
    "Liang Wei, UNI:lw2716\n",
    "\n",
    "**a)**\n",
    "```python\n",
    "a = np.pi\n",
    "b = 3.14\n",
    "print a, b, a-b, (a-b)/a\n",
    "```\n",
    "\n",
    "absolute error = $|f-\\hat f| = |\\pi - 3.14|$ = 0.00159265358979\n",
    "\n",
    "relative error = $\\frac{error_{absolute}}{\\pi}$ = 0.000506957382897\n",
    "   \n",
    "absolute decimal precision = 3\n",
    "   \n",
    "**b)**  \n",
    "```python\n",
    "a = np.pi\n",
    "b = 22.0/7.0\n",
    "print a, b, a - b, (a-b)/a\n",
    "```\n",
    "absolute error = $|f - \\hat f| = |\\pi - \\frac{22}{7}|$ = 0.00126448926735\n",
    "\n",
    "relative error = $\\frac{absolute error}{\\pi}$ = 0.000402499434771\n",
    "   \n",
    "decimal precision = 3\n",
    "   \n",
    "**c)** \n",
    "```python\n",
    "import scipy.misc as misc\n",
    "a = np.log(misc.factorial(100.0))\n",
    "b = 100.0 * np.log(100.0) - 100.0\n",
    "print a, b, a-b, (a - b)/a\n",
    "```\n",
    "\n",
    "**for n = 5:**\n",
    "\n",
    "absolute error = $|f - \\hat f| = |log(5!) - (5 \\times log(5) - 5)|$ = 1.74030218061\n",
    "\n",
    "relative error = $\\frac{absolute error}{log(5!)}$ = 0.363510220824\n",
    "              \n",
    "absolute decimal precision = 0, relative decimal precision = 0\n",
    "              \n",
    "**for n = 10:** \n",
    "   \n",
    "absolute error = $|f - \\hat f| = |log(10!) - (10 \\times log(10) - 10)|$ = 2.07856164314\n",
    "   \n",
    "relative error = $\\frac{absolute error}{log(10!)}$= 0.137612875249\n",
    "               \n",
    "absolute decimal precision = 1, relative decimal precision = 0\n",
    "               \n",
    "**for n = 100:** \n",
    "   \n",
    "absolute error = $|f - \\hat f| = |log(100!) - (100 \\times log(100) - 100)|$ = 3.22235695675\n",
    "   \n",
    "relative error = $\\frac{absolute error}{log(100!)}$ = 0.00885897203687\n",
    "                \n",
    "absolute decimal precision = 2, relative decimal precision = 2\n",
    "   \n",
    "   \n",
    "   \n",
    "**d)**\n",
    "```python\n",
    "import sympy\n",
    "import scipy.misc as misc\n",
    "def lamda(x):\n",
    "    n = 1\n",
    "    Tn = 1\n",
    "    while n <= 100:\n",
    "        Tn = Tn + x**n/misc.factorial(n)\n",
    "        if np.exp(x) - Tn < 0.000009:\n",
    "            return n, Tn\n",
    "        else:\n",
    "            n = n + 1\n",
    "    else:\n",
    "        print x\n",
    "        \n",
    "    lamda(1)\n",
    "```\n",
    "```python\n",
    "a = np.exp(1)\n",
    "b = lamda(1)\n",
    "absolute error = np.absolute(a - b)/a\n",
    "```\n",
    "**$T_{1}(1)$** \n",
    "\n",
    "absolute error = $|f - \\hat f| = |e^{1} - T_{1}(1)|$ = 0.718281828459\n",
    "   \n",
    "relative error = $\\frac{absolute error}{f}$ = 0.264241117657\n",
    "   \n",
    "decimal precision = 1\n",
    "   \n",
    "**$T_{1}(2)$**\n",
    "   \n",
    "absolute error = $|f - \\hat f| = |e^{1} - T_{1}(2)|$  = 0.218281828459\n",
    "   \n",
    "relative error = $\\frac{absolute error}{f}$ = 0.0803013970714\n",
    "   \n",
    "decimal precision = 1\n",
    "   \n",
    "**$T_{1}(3)$**\n",
    "   \n",
    "absolute error = $|f - \\hat f| = |e^{1} - T_{1}(3)|$ = 0.0516151617924\n",
    "   \n",
    "relative error = $\\frac{absolute error}{f}$ = 0.0189881568762\n",
    "   \n",
    "decimal precision = 2\n",
    "   \n",
    "If we want to get decimal precision as good as 6 digits, when we take x = 1, the N would be 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-a",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(a)** (10) Write a Python program to compute\n",
    "\n",
    "$$S_N = \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ] = \\sum^N_{n=1} \\frac{1}{n (n + 1)}$$\n",
    "\n",
    "once using the first summation and once using the second for $N = 10, 10^2, \\ldots , 10^7$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "A2-a",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sum_1(N):\n",
    "    \"\"\"Compute the summation S_N defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ]\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns Sn (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    Sn = 0.0\n",
    "    Sn += sum(1.0/n - 1.0/(n + 1) for n in xrange(1, N + 1))\n",
    "    return Sn\n",
    "\n",
    "\n",
    "def sum_2(N):\n",
    "    \"\"\"Compute the summation S_N defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\frac{1}{n (n + 1)}\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns Sn (float)\n",
    "    \"\"\"\n",
    "\n",
    "    # INSERT CODE HERE\n",
    "    Sn = 0.0\n",
    "    Sn += sum(1.0/(n * (n + 1)) for n in xrange(1, N + 1))\n",
    "    return Sn\n",
    "#print sum_1(10)\n",
    "#print sum_2(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T2-a",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "N = numpy.array([10**n for n in xrange(1,8)])\n",
    "answer = numpy.zeros((2, N.shape[0]))\n",
    "for (n, upper_bound) in enumerate(N):\n",
    "    answer[0, n] = sum_1(upper_bound)\n",
    "    answer[1, n] = sum_2(upper_bound)\n",
    "numpy.testing.assert_allclose(answer[0, :], numpy.array([0.9090909090909089, 0.9900990099009896, \n",
    "                                                         0.9990009990009996, 0.9999000099990004, \n",
    "                                                         0.9999900001000117, 0.9999990000010469,\n",
    "                                                         0.9999998999998143]))\n",
    "numpy.testing.assert_allclose(answer[1, :], numpy.array([0.9090909090909091, 0.9900990099009898, \n",
    "                                                         0.9990009990009997, 0.9999000099990007, \n",
    "                                                         0.9999900001000122, 0.9999990000010476, \n",
    "                                                         0.9999998999998153]))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-b",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(b)**  (5) Compute the absolute error between the two summation approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "A2-b",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def abs_error(N):\n",
    "    \"\"\"Compute the absolute error of the two sums defined as\n",
    "    \n",
    "    \\sum^N_{n=1} \\left [ \\frac{1}{n} - \\frac{1}{n+1} \\right ]\n",
    "    \n",
    "    and \n",
    "    \n",
    "    \\sum^N_{n=1} \\frac{1}{n (n + 1)}\n",
    "    \n",
    "    respectively for the given N.\n",
    "    \n",
    "    :Input:\n",
    "     *N* (int) The upper bound on the summation.\n",
    "    \n",
    "    Returns *error* (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    abs_error = numpy.absolute(sum_1(N) - sum_2(N))\n",
    "    return abs_error\n",
    "#print abs_error(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T2-b",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "N = numpy.array([10**n for n in xrange(1,8)])\n",
    "answer = numpy.zeros(N.shape)\n",
    "for (n, upper_bound) in enumerate(N):\n",
    "    answer[n] = abs_error(upper_bound)\n",
    "numpy.testing.assert_allclose(answer, numpy.array([1.1102230246251565e-16, 1.1102230246251565e-16, \n",
    "                                                   1.1102230246251565e-16, 3.3306690738754696e-16, \n",
    "                                                   4.4408920985006262e-16, 6.6613381477509392e-16, \n",
    "                                                   9.9920072216264089e-16]))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-c",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(c)** (10) Plot the relative and absolute error versus $N$.  Also plot a line where $\\epsilon_{\\text{machine}}$ should be.  Comment on what you see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "A2-c",
     "locked": false,
     "points": 10,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAEWCAYAAAAjCPKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ca34a50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(fig.get_figwidth() * 2)\n",
    "\n",
    "# HINT!  Use the plotting function semilogx to plot the errors\n",
    "#        Also, do not forget to label your plot\n",
    "\n",
    "# INSERT CODE HERE\n",
    "\n",
    "x = [10, 100, 1000, 10000, 100000, 1000000, 10000000]\n",
    "\n",
    "\n",
    "def sum_1(N):\n",
    "    \n",
    "    N = numpy.array([10**n for n in xrange(1, 8)])\n",
    "    Sn = []\n",
    "    for (n, upper_bound) in enumerate(N):\n",
    "        Sn.append(numpy.sum((1.0/i) - (1.0/(i + 1)) for i in xrange(1, upper_bound + 1)))\n",
    "    return Sn\n",
    "\n",
    "result_1 = sum_1(10)\n",
    "\n",
    "def sum_2(N):\n",
    "    N = numpy.array([10**n for n in xrange(1, 8)])\n",
    "    Sn = []\n",
    "    for (n, upper_bound) in enumerate(N):\n",
    "        Sn.append(numpy.sum((1.0/(i*(i+1)) for i in xrange(1, upper_bound + 1))))\n",
    "    return Sn\n",
    "\n",
    "result_2 = sum_2(10)\n",
    "\n",
    "absolute_error = numpy.absolute(numpy.subtract(result_1, result_2))\n",
    "relative_error = absolute_error/result_1\n",
    "\n",
    "\n",
    "axes = fig.add_subplot(1, 1, 1)\n",
    "axes.loglog(x, absolute_error, label = \"absolute error\")\n",
    "axes.loglog(x, relative_error, label = \"relative error\")\n",
    "\n",
    "epsilon = numpy.finfo(float).eps\n",
    "machine_epsilon = [epsilon, epsilon, epsilon, epsilon, epsilon, epsilon, epsilon]\n",
    "axes.loglog(x, machine_epsilon, label = \"machine epsilon\")\n",
    "axes.legend(loc=2)            \n",
    "axes.set_xlabel(\"N\")\n",
    "axes.set_ylabel(\"Error\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: when N is small, the error is also small. When N goes bigger, the error goes bigger. And it increases fast\n",
    "    during $10^3$ and $10^4$. Later, the rate of increase decreased, but it still goes bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q2-d",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(d)** (5) Theorize what may have lead to the differences in answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A2-d",
     "locked": false,
     "points": 5,
     "solution": true
    }
   },
   "source": [
    "When N becomes bigger, the absolute error goes bigger. Since everytime we calculate $\\frac{1}{n}$, $\\frac{1}{n + 1}$ and $\\frac{1}{n \\times (n +1)}$ they would lose accruary, the error finally will accumulated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Following our discussion in lecture regarding approximating $e^x$ again consider the Taylor polynomial approximation:\n",
    "\n",
    "$$e^x \\approx T_n(x) = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots + \\frac{x^n}{n!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-a",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(a)** Derive the upper bound on the *relative error* assuming that $x > 0$ and\n",
    "\n",
    "$$R_n = \\frac{|e^x - T_n(x)|}{|e^x|}$$\n",
    "\n",
    "is given by\n",
    "\n",
    "$$R_n \\leq \\left | \\frac{x^{n+1}}{(n + 1)!} \\right |$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A3-a",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": [
    "Let's define $E(x)$ is the difference (absolute error) between $e^{x}$ and Taylor polynomial approximation $T_{n}(x)$. So, $$E(x) = |e^{x} - T_{n}(x)|$$\n",
    "\n",
    "the relative error $$R_{n} = |\\frac{E(x)}{f(x)}|$$, $$f(x) = e^{x}$$\n",
    "\n",
    "First, when $x = 0$, $e^{0} = 1$, $T_{n}(0) = 1$, $E_{x} = 0$. At $x = 0$, if we take $n$ derivative of both sides, we have $E^{(n)}(x) = f^{(n)}(x) - T^{(n)}_{n}(x) = 0$. \n",
    "\n",
    "Furthermore, if we take $n+1$ derivative of both sides, let's bound to $$E^{(n+1)}(x)\\leq M$$\n",
    "\n",
    "we have $$E^{(n+1)}(x) = f^{(n+1)}(x) - T^{(n+1)}_{n}(x) \\leq M$$\n",
    "\n",
    "In this equation, since $T_{n}$ only has $n$ degree, thus, $$T^{(n+1)}_{n} = 0$$ \n",
    "\n",
    "We have  $$f^{(n+1)}(x) \\leq M$$\n",
    "\n",
    "Then, we intergal both side, we will get $$E^{(n)}(x) = f^{(n)}(x) \\leq Mx + C$$\n",
    "\n",
    "We want to choose C as minimum as possible. So, if we put $x = 0$ here, $0\\leq C$, we choose $C = 0$. Keep doing this for n times. We will have $$E(x) \\leq \\frac{Mx^{n+1}}{(n + 1)!} $$\n",
    "\n",
    "Now, the relative error $$R_{n} = |\\frac{E(x)}{e^{x}}| \\leq E(x)$$ since $|e^{x}|>0$ all the time. \n",
    "\n",
    "Thus, $$R_{n} \\leq M|\\frac{x^{n+1}}{(n+1)!}|$$\n",
    "\n",
    "Since $e^{x}$ could be any number bigger than 0, if we choose $M = e^{x}$, we will have $$R_{n} \\leq |\\frac{x^{n+1}}{(n+1)!}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-b",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "**(b)** Show that for large $x$ and $n$, $r_n \\leq \\epsilon_{\\text{machine}}$ implies that we need at least $n > e \\cdot x$ terms in the series (where $e = \\text{exp}(1)$).\n",
    "\n",
    "*Hint* Use Stirling's approximation $log (n!) \\approx n~log~n - n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A3-b",
     "locked": false,
     "points": 15,
     "solution": true
    }
   },
   "source": [
    "From (a), we have $$R_{n} \\leq |\\frac{x^{n+1}}{(n+1)!}|$$ If we can prove $$|\\frac{x^{n+1}}{(n+1)!}| \\leq \\epsilon$$ We can prove $$R_{n} \\leq \\epsilon$$ \n",
    "\n",
    "We times $(n + 1)!$ on both sides, then we get $$x^{n+1} \\leq \\epsilon_{machine}(n + 1)!$$ \n",
    "\n",
    "We put $\\log$ on each side, since $\\log$ function is monotonic, we have $$(n+1)\\log(x) \\leq \\log(\\epsilon) + \\log(n+1)!$$\n",
    "\n",
    "Next, from Stirling's approximation, we have $$\\log(n+1)! \\approx (n+1)\\log(n+1) - (n+1)$$\n",
    "\n",
    "Thus, we have $$(n+1)\\log(x) \\leq \\log(\\epsilon) + (n+1)\\log(n+1) - (n+1)$$\n",
    "\n",
    "Divide $n+1$ on each side, we have $$\\log(x) \\leq \\frac{\\log(\\epsilon)}{n+1} + \\log(n+1) - 1$$\n",
    "\n",
    "Since $\\log(\\epsilon)$ is very small, especially divided by $n+1$ when n is a large number. This item gone. \n",
    "\n",
    "Also, when n is large, $\\log(n) \\approx \\log(n+1)$ \n",
    "\n",
    "Then, we have $$1 \\leq \\log(n) - \\log(x)$$\n",
    "\n",
    "which implies $$1 \\leq \\log(\\frac{n}{x})$$\n",
    "\n",
    "Here, we take exponential of them and get $$e^1 \\leq e^{\\log(\\frac{n}{x})}$$ \n",
    "\n",
    "We have $$e \\leq \\frac{n}{x}$$ \n",
    "\n",
    "Then we get $$n \\geq e \\cdot x$$\n",
    "\n",
    "So, because $R_{n} \\leq |\\frac{x^{n+1}}{(n+1)!}|$ we proved if $R_{n} \\leq \\epsilon$ we need at least $n \\geq e \\cdot x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q3-c",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**(c)** Write a Python function that accurately computes $T_n$ to the specified relative error tolerance and returns both the estimate on the range and the number of terms in the series needed over the interval $[-2, 2]$.  Note that the testing tolerance will be $8 \\cdot \\epsilon_{\\text{machine}}$.\n",
    "\n",
    "Make sure to document your code including expected inputs, outputs, and assumptions being made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "A3-c",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# HINT: Think about how we evaluated polynomials efficiently in class\n",
    "\n",
    "import scipy.misc as misc\n",
    "import sympy\n",
    "\n",
    "def Tn_exp(x, tolerance=1e-3):\n",
    "    \"\"\"This function takes x as input, compute how many terms we need to get T(n) close to e^x\n",
    "    \n",
    "    with an acceptable relative error. It returns both the estimate on the range and the number\n",
    "    \n",
    "    of terms in the series needed over the interval [-2, 2].\n",
    "    \n",
    "    \"\"\"\n",
    "    MAX_N = 100\n",
    "    \n",
    "    # INSERT CODE HERE\n",
    "    n = 1\n",
    "    Tn = 1.0\n",
    "    while n <= MAX_N:\n",
    "        Tn = Tn + x**n/misc.factorial(n)\n",
    "        abs_error = numpy.absolute(numpy.exp(x) - Tn)\n",
    "        rel_error = abs_error / numpy.exp(x)\n",
    "        if numpy.all(rel_error < tolerance):\n",
    "            break\n",
    "        else:\n",
    "            n = n + 1\n",
    "    else: \n",
    "        print \"n reach max\"\n",
    "    \n",
    "    return Tn, n\n",
    "#Tn_exp(2, tolerance = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "T3-c",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "x = numpy.linspace(-2, 2, 100)\n",
    "tolerance = 8.0 * numpy.finfo(float).eps\n",
    "answer, N = Tn_exp(x, tolerance=tolerance)\n",
    "assert(numpy.all(numpy.abs(answer - numpy.exp(x)) / numpy.abs(numpy.exp(x)) < tolerance))\n",
    "print \"Success!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "Q4",
     "locked": true,
     "points": 1,
     "solution": false
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "Given the Taylor polynomial expansions\n",
    "\n",
    "$$\\frac{1}{1-\\Delta x} = 1 + \\Delta x + \\Delta x^2 + \\Delta x^3 + \\mathcal{O}(\\Delta x^4)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\cosh \\Delta x = 1 - \\frac{\\Delta x^2}{2!} + \\frac{\\Delta x^4}{4!} + \\mathcal{O}(\\Delta x^6)$$\n",
    "\n",
    "determine the order of approximation for their sum and product (determine the exponent that belongs in the $\\mathcal{O}$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "A4",
     "locked": false,
     "points": 10,
     "solution": true
    }
   },
   "source": [
    "From the theorem on class, we have\n",
    "\n",
    "$$f(\\Delta x) = p(\\Delta x) + O(\\Delta x^n)$$\n",
    "$$g(\\Delta x) = q(\\Delta x) + O(\\Delta x^m)$$\n",
    "$$r = \\min(n, m)$$ then\n",
    "\n",
    "$$f+g = p + q + O(\\Delta x^r)$$\n",
    "\n",
    "$$f \\cdot g = p \\cdot q + p \\cdot O(\\Delta x^m) + q \\cdot O(\\Delta x^n) + O(\\Delta x^{n+m}) = p \\cdot q + O(\\Delta x^r)$$\n",
    "\n",
    "so when we add them up, define $$f(\\Delta x) = \\frac{1}{1-\\Delta x}$$ and $$g(\\Delta x) = cosh\\Delta x$$\n",
    "\n",
    "we have $$f+g = 2 + \\Delta x - \\frac{\\Delta x^2}{2} + \\Delta x^3 + \\mathcal{O}(\\Delta x^4) + \\mathcal{O}(\\Delta x^6)$$ Since $\\Delta x$ is a small number, $$\\Delta x^4 \\geq \\Delta x^6$$ \n",
    "\n",
    "In this case, the order of approximation for their sum is 4.\n",
    "\n",
    "When we multiply them up, we have $$f \\cdot g = 1 + \\Delta x + \\frac{3 \\Delta x^2}{2} + \\frac{\\Delta x^3}{2} + \\mathcal{O}(\\Delta x^4) - \\frac{\\Delta x^5}{4} + \\mathcal{O}(\\Delta x^6) + \\mathcal{O}(\\Delta x^7) + \\Delta x^9 + \\Delta x^{10} $$ In this case, since $\\Delta x$ is a small number, $\\mathcal{O}(x^4)$ is the biggest big-O item, thus, the order of approximation for their product is also 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
